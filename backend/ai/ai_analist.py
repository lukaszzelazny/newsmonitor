import os
import json
import numpy as np
from openai import OpenAI
from dotenv import load_dotenv
from collections import defaultdict
from backend.database import Database, NewsArticle, AnalysisResult, TickerSentiment, Ticker, \
    SectorSentiment, BrokerageAnalysis, NewsNotAnalyzed
from backend.tools.normalizer import get_normalizer
from sklearn.metrics.pairwise import cosine_similarity
from sqlalchemy import text

normalizer = get_normalizer()

load_dotenv()

client = OpenAI(api_key=os.getenv('OPENAI_API', ''))

def load_patterns(filepath='patterns.json', name="relevant_patterns"):
    """Wczytuje atrybut 'relevant_patterns' z pliku JSON"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if name in data:
                return data[name]
            else:
                print(f"Brak klucza {name} w {filepath}, u≈ºywam domy≈õlnych wzorc√≥w")
                return None
    except FileNotFoundError:
        print(f"Brak pliku {filepath}, u≈ºywam domy≈õlnych wzorc√≥w")
        return None


# Wzorcowe frazy dla r√≥≈ºnych kategorii istotnych news√≥w
RELEVANT_PATTERNS = load_patterns(name="revelant_patterns")
# Nieistotne wzorce
IRRELEVANT_PATTERNS = load_patterns(name="irrevelant_patterns")
NEGATIVE_KEYWORDS = load_patterns(name="negative_keywords")

NEWS_SUMMARY_PATTERN = load_patterns(name="summary_patterns")

def get_embedding(text: str, model: str = "text-embedding-3-large"):
    """
    Pobiera embedding dla danego tekstu.

    Args:
        text: Tekst do embedowania
        model: Model embeddings

    Returns:
        Lista float - wektor embedingu
    """
    text = text.replace("\n", " ").strip()
    if not text:
        return None

    try:
        response = client.embeddings.create(input=[text], model=model)
        return response.data[0].embedding
    except Exception as e:
        print(f"B≈ÇƒÖd podczas generowania embeddingu: {e}")
        return None


def calculate_relevance_score(news_embedding, pattern_embeddings):
    """
    Oblicza score istotno≈õci na podstawie podobie≈Ñstwa cosine.

    Args:
        news_embedding: Embedding newsa
        pattern_embeddings: Lista embeddings wzorc√≥w

    Returns:
        Float - maksymalne podobie≈Ñstwo (0-1)
    """
    if news_embedding is None or not pattern_embeddings:
        return 0.0

    news_emb = np.array(news_embedding).reshape(1, -1)

    max_similarity = 0.0
    for pattern_emb in pattern_embeddings:
        if pattern_emb is not None:
            pattern_arr = np.array(pattern_emb).reshape(1, -1)
            similarity = cosine_similarity(news_emb, pattern_arr)[0][0]
            max_similarity = max(max_similarity, similarity)

    return float(max_similarity)


def contains_pattern(pattern: list, title: str, content: str) -> tuple[bool, str]:
    """
    Sprawdza czy news zawiera s≈Çowa kluczowe z listy pattern.
    
    Args:
        title: Tytu≈Ç artyku≈Çu
        content: Tre≈õƒá artyku≈Çu
    
    Returns:
        Tuple[bool, str] - (czy_zawiera, znalezione_s≈Çowo_kluczowe)
    """
    if not pattern:
        return False, ""
    
    # ≈ÅƒÖczymy tytu≈Ç i tre≈õƒá w jeden tekst
    full_text = f"{title} {content or ''}".lower()
    
    # Sprawdzamy ka≈ºde s≈Çowo kluczowe
    for keyword in pattern:
        if keyword.lower() in full_text:
            return True, keyword
    
    return False, ""


def is_news_relevant(headline: str, lead: str, threshold: float = 0.65):
    """
    Sprawdza czy news jest istotny przy u≈ºyciu embeddings.

    Args:
        headline: Tytu≈Ç artyku≈Çu
        lead: Tre≈õƒá artyku≈Çu
        threshold: Pr√≥g istotno≈õci (0-1)

    Returns:
        Tuple[bool, float, str] - (czy_istotny, score, pow√≥d)
    """
    # Po≈ÇƒÖcz tytu≈Ç i lead
    full_text = f"{headline}. {lead}"

    # Pobierz embedding newsa
    news_embedding = get_embedding(full_text)
    if news_embedding is None:
        return False, 0.0, "B≈ÇƒÖd generowania embeddingu"

    # Generuj embeddingi dla wzorc√≥w istotnych (cachowane w pamiƒôci)
    if not hasattr(is_news_relevant, '_relevant_cache'):
        print("Generujƒô embeddingi wzorc√≥w istotnych...")
        is_news_relevant._relevant_cache = {}
        for category, patterns in RELEVANT_PATTERNS.items():
            is_news_relevant._relevant_cache[category] = [
                get_embedding(pattern) for pattern in patterns
            ]

    # Generuj embeddingi dla wzorc√≥w nieistotnych
    if not hasattr(is_news_relevant, '_irrelevant_cache'):
        print("Generujƒô embeddingi wzorc√≥w nieistotnych...")
        is_news_relevant._irrelevant_cache = [
            get_embedding(pattern) for pattern in IRRELEVANT_PATTERNS
        ]

    # Oblicz score dla kategorii istotnych
    category_scores = {}
    for category, embeddings in is_news_relevant._relevant_cache.items():
        score = calculate_relevance_score(news_embedding, embeddings)
        category_scores[category] = score

    max_relevant_score = max(category_scores.values()) if category_scores else 0.0
    best_category = max(category_scores,
                        key=category_scores.get) if category_scores else None

    # Oblicz score dla wzorc√≥w nieistotnych
    irrelevant_score = calculate_relevance_score(
        news_embedding,
        is_news_relevant._irrelevant_cache
    )

    # Decyzja
    if irrelevant_score > 0.70:
        return False, irrelevant_score, f"Wykryto nieistotny wzorzec (score: {irrelevant_score:.3f})"

    if max_relevant_score >= threshold:
        return True, max_relevant_score, f"Kategoria: {best_category} (score: {max_relevant_score:.3f})"

    return False, max_relevant_score, f"{max_relevant_score:.3f} < {threshold}, {best_category}"


def save_not_analyzed(db: Database, news_id: int, reason: str, relevance_score: float):
    """
    Zapisuje informacjƒô o newsie, kt√≥ry nie zosta≈Ç przeanalizowany.

    Args:
        db: Instancja Database
        news_id: ID artyku≈Çu
        reason: Pow√≥d nieprzeanalizowania
        relevance_score: Score istotno≈õci
    """
    session = db.Session()
    try:
        session.execute(
            text("""
            INSERT INTO news_not_analyzed (news_id, reason, relevance_score)
            VALUES (:news_id, :reason, :relevance_score)
            ON CONFLICT (news_id) DO NOTHING
            """),
            {"news_id": news_id, "reason": reason, "relevance_score": relevance_score}
        )
        session.commit()
        print(f"‚úì Zapisano do news_not_analyzed: ID={news_id}, pow√≥d: {reason}")
    except Exception as e:
        session.rollback()
        print(f"‚úó B≈ÇƒÖd zapisu do news_not_analyzed: {e}")
    finally:
        session.close()


PROMPT_NEWS = """
Jeste≈õ do≈õwiadczonym analitykiem gie≈Çdowym.
Twoim zadaniem jest analizowaƒá wiadomo≈õci ekonomiczne, gie≈Çdowe i biznesowe
(np. z serwisu PAP Biznes) oraz oceniaƒá ich potencjalne znaczenie rynkowe.

**KLUCZOWA ZASADA**: Wiadomo≈õƒá mo≈ºe wp≈Çywaƒá na:
- Konkretne sp√≥≈Çki (ticker_impact)
- Ca≈Çy sektor (sector_impact)
- Oba jednocze≈õnie

Zasady analizy:
{ticker_context}

1. **Rozpoznaj typ wiadomo≈õci**:
   - üè¢ Sp√≥≈Çka
   - üè≠ Sektor
   - üí∞ IPO / Debiut
   - üìä Makro
   - üìâ Neutralna

2. **Zidentyfikuj tickery**:
   - Je≈õli wiadomo≈õƒá dotyczy sp√≥≈Çek ‚Üí podaj tickery
   - W innym przypadku ‚Üí []

3. **ticker_impact vs sector_impact**:
   - ticker_impact gdy dotyczy sp√≥≈Çek
   - sector_impact gdy dotyczy bran≈ºy
   - oba, gdy wystƒôpujƒÖ oba poziomy wp≈Çywu

4. **KRYTYCZNE ‚Äì emisje akcji, ABB i wyjƒÖtek sell + new issue**

   **STANDARDOWA ZASADA (domy≈õlna):**
   - ABB ‚Üí ZAWSZE silnie negatywne (ticker_impact -0.6 do -0.8)
   - Klasyczna emisja nowych akcji ‚Üí negatywna (rozwodnienie + zwiƒôkszona poda≈º)

   **WYJƒÑTEK, KT√ìRY MUSISZ BEZWZGLƒòDNIE UWZGLƒòDNIƒÜ:**
   Je≈õli transakcja ma strukturƒô typu **"sell + new issue"**, czyli:
   - istniejƒÖcy akcjonariusz sprzedaje pakiet inwestorowi instytucjonalnemu,
   - nowa emisja jest wy≈ÇƒÖcznie techniczna i s≈Çu≈ºy odtworzeniu jego stanu posiadania,
   - **nie zwiƒôksza siƒô liczba akcji w wolnym obrocie**,
   - **nie dochodzi do realnego rozwodnienia**, 
   - inwestor instytucjonalny sygnalizuje zaufanie do sp√≥≈Çki,
   - pozyskane ≈õrodki finansujƒÖ rozw√≥j,

   TO:
   - **nie wolno traktowaƒá tego jako emisji negatywnej**,  
   - **ticker_impact nie mo≈ºe byƒá ujemny**,  
   - minimalny dopuszczalny wynik to **+0.2**,  
   - traktuj to jako informacjƒô neutralno‚ÄìpozytywnƒÖ lub pozytywnƒÖ.

5. **Wyceny dom√≥w maklerskich**:
   Je≈õli wystƒôpujƒÖ:
     - brokerage_house
     - price_old
     - price_new
     - price_recomendation
     - price_comment
   Je≈õli brak ‚Üí null

6. **Oceniaj wp≈Çyw**:
   - ticker_impact lub sector_impact (jedna liczba)
   - confidence 0‚Äì1
   - occasion: kr√≥tkoterminowa / ≈õrednioterminowa / d≈Çugoterminowa lub null

7. **Skalowanie wp≈Çywu**:
   - -1.0 do -0.7: bardzo negatywne
   - -0.6 do -0.3: negatywne
   - -0.2 do +0.2: neutralne
   - +0.3 do +0.6: pozytywne
   - +0.7 do +1.0: bardzo pozytywne

8. **Dodaj kr√≥tkie uzasadnienie** w polu "reason".

9. **FORMAT ODPOWIEDZI**:
   - Zwr√≥ƒá wy≈ÇƒÖcznie poprawny JSON
   - BEZ komentarzy, BEZ markdown
---

### Wej≈õcie:
News:
"{headline}"
"{lead}"

### Oczekiwany wynik:
Zwr√≥ƒá wy≈ÇƒÖcznie **poprawny JSON** w formacie:

{{
  "typ": "<Sektor / Sp√≥≈Çka / Makro / IPO / Neutralna>",
  "related_tickers": ["..."],
  "sector": "<nazwa sektora lub null>",
  "ticker_impact": <POJEDYNCZA liczba lub null>,
  "sector_impact": <liczba lub null>,
  "confidence": <liczba lub null>,
  "occasion": "<typ okazji lub null>",
  "reason": "<kr√≥tkie wyja≈õnienie>",
  "brokerage_house": "<nazwa domu maklerskiego lub null>",
  "price_old": "<stara wycena lub null>",
  "price_new": "<nowa wycena lub null>",
  "price_recomendation": "<rekomendacja lub null>",
  "price_comment": "<komentarz do wyceny lub null>"
}}
"""


PROMPT_SUMMARY_FIXED = """
üß† Prompt PRO ‚Äî analiza podsumowania dnia (zbioru news√≥w)

Jeste≈õ do≈õwiadczonym analitykiem gie≈Çdowym.
Twoim zadaniem jest analizowaƒá zbiorcze podsumowania wiadomo≈õci ekonomicznych, gie≈Çdowych i biznesowych (np. z serwisu PAP Biznes lub Strefa Inwestor√≥w) i oceniaƒá potencjalne znaczenie poszczeg√≥lnych informacji rynkowych.

Tekst, kt√≥ry otrzymasz, mo≈ºe zawieraƒá wiele kr√≥tkich news√≥w lub streszcze≈Ñ w jednym artykule. Ka≈ºdy z nich potraktuj jako osobny wpis.
Dla ka≈ºdego fragmentu (newsa) zastosuj poni≈ºsze zasady analizy i zwr√≥ƒá listƒô obiekt√≥w JSON ‚Äì po jednym dla ka≈ºdej istotnej informacji.

Zasady analizy:
{ticker_context}

1. **Rozpoznaj typ wiadomo≈õci**:
   - üè¢ Sp√≥≈Çka ‚Äì dotyczy konkretnego podmiotu lub kilku sp√≥≈Çek
   - üè≠ Sektor ‚Äì odnosi siƒô do ca≈Çej bran≈ºy (np. banki, energetyka, gaming)
   - üí∞ Debiut / IPO ‚Äì informacja o wej≈õciu sp√≥≈Çki na gie≈Çdƒô
   - üìä Makro / Rynek ‚Äì dotyczy zjawisk gospodarczych, wska≈∫nik√≥w, polityki pieniƒô≈ºnej, cen surowc√≥w, decyzji NBP/FED itp.
   - üìâ NiepowiƒÖzana / Neutralna ‚Äì nie ma znaczenia dla rynku lub kurs√≥w akcji

   **WA≈ªNE - Emisja nowych akcji (ABB):**
    - Oce≈Ñ wp≈Çyw opisanej transakcji na kurs akcji, biorƒÖc pod uwagƒô, ≈ºe: 
    Sp√≥≈Çka przeprowadzi≈Ça transakcjƒô typu ‚Äûsell + new issue‚Äù, w kt√≥rej istniejƒÖcy akcjonariusz sprzeda≈Ç pakiet akcji inwestorowi instytucjonalnemu.
    - Nowe akcje zostanƒÖ wyemitowane wy≈ÇƒÖcznie po to, aby odkupiƒá je przez tego samego akcjonariusza, 
    wiƒôc nie zwiƒôkszy siƒô liczba akcji w wolnym obrocie i nie wystƒÖpi realne rozwodnienie.
    - Cena transakcyjna z inwestorem instytucjonalnym zosta≈Ça ustalona powy≈ºej 
    ≈õredniej z wycen rynku lub w spos√≥b wskazujƒÖcy na zaufanie inwestora.
    - Sp√≥≈Çka pozyskuje znaczƒÖcy kapita≈Ç na rozw√≥j, 
    co poprawia jej mo≈ºliwo≈õci inwestycyjne, a inwestor instytucjonalny daje pozytywny sygna≈Ç rynkowi.

2. **Zidentyfikuj tickery**:
   - Je≈ºeli wiadomo≈õƒá dotyczy konkretnych sp√≥≈Çek, wypisz ich tickery (np. "related_tickers": ["KGH", "PZU"])
   - Je≈õli brak ‚Äî zwr√≥ƒá pustƒÖ listƒô: "related_tickers": []

3. **WA≈ªNE - ticker_impact**:
   - `ticker_impact` MUSI byƒá POJEDYNCZƒÑ liczbƒÖ od -1.0 do +1.0
   - Reprezentuje ≈öREDNI wp≈Çyw na wszystkie wymienione sp√≥≈Çki
   - Je≈õli sp√≥≈Çki majƒÖ r√≥≈ºny wp≈Çyw, oblicz ≈õredniƒÖ wa≈ºonƒÖ
   - NIE u≈ºywaj obiektu z r√≥≈ºnymi warto≈õciami dla ka≈ºdego tickera
   
4. **KRYTYCZNE - ABB (Accelerated Book Building) i nowe emisje akcji**:
   - **ABB to przyspieszona sprzeda≈º du≈ºego pakietu akcji** (zwykle z dyskontem 5-10%)
   - Wiadomo≈õci o ABB majƒÖ **WYSOKI NEGATYWNY WP≈ÅYW** (ticker_impact od -0.6 do -0.8)
   - Pow√≥d: dyskonto w cenie + obawa o brak perspektyw + zwiƒôkszona poda≈º
   - **Nowe emisje akcji** (podwy≈ºszenie kapita≈Çu) r√≥wnie≈º majƒÖ **negatywny wp≈Çyw**
   - Pow√≥d: rozwodnienie udzia≈Ç√≥w istniejƒÖcych akcjonariuszy + zwiƒôkszona poda≈º
   - WyjƒÖtek: je≈õli emisja s≈Çu≈ºy strategicznej akwizycji i jest dobrze odbierana przez rynek
   - Zwracaj szczeg√≥lnƒÖ uwagƒô na s≈Çowa kluczowe: "ABB", "przyspieszona budowa ksiƒôgi", "emisja akcji", "podwy≈ºszenie kapita≈Çu", "new stock offering"

5. **Uwzglƒôdnij nowe wyceny od dom√≥w maklerskich (DM)**:
   - Je≈õli wystƒôpuje informacja o rekomendacji lub zmianie wyceny, wypisz:
     - "brokerage_house" ‚Äì nazwa domu maklerskiego
     - "price_old" ‚Äì stara wycena
     - "price_new" ‚Äì nowa wycena
     - "price_recomendation" ‚Äì np. "kupuj", "neutralnie", "sprzedaj"
     - "price_comment" ‚Äì kr√≥tki opis komentarza
     - "reason" ‚Äì uzasadnienie wp≈Çywu tej zmiany
   - Je≈õli brak danych o wycenach ‚Äî wpisz warto≈õci null

6. **Oce≈Ñ wp≈Çyw wiadomo≈õci**:
   - Je≈õli dotyczy sp√≥≈Çki/sp√≥≈Çek:
     - "ticker_impact" ‚Äì POJEDYNCZA liczba od -1.0 do +1.0 (≈õredni wp≈Çyw)
     - "confidence" ‚Äì liczba od 0.0 do 1.0
     - "occasion" ‚Äì "kr√≥tkoterminowa", "≈õrednioterminowa", "d≈Çugoterminowa"
     - "sector" ‚Äì nazwa sektora
     - "sector_impact" ‚Äì null
   - Je≈õli dotyczy ca≈Çego sektora:
     - "sector" ‚Äì nazwa sektora
     - "sector_impact" ‚Äì liczba od -1.0 do +1.0
     - "confidence" ‚Äì liczba od 0.0 do 1.0
     - "occasion" ‚Äì null
     - "ticker_impact" ‚Äì null
   - Je≈õli wiadomo≈õƒá neutralna:
     - wszystkie pola wp≈Çywu (ticker_impact, sector_impact, confidence, occasion, sector) majƒÖ warto≈õƒá null

7. **Dodaj kr√≥tkie uzasadnienie** ("reason") ‚Äì jedno lub dwa zdania wyja≈õniajƒÖce, dlaczego dana informacja mo≈ºe (lub nie mo≈ºe) wp≈ÇynƒÖƒá na rynek

8. **FORMAT ODPOWIEDZI**:
   - Zwr√≥ƒá TYLKO czystƒÖ tablicƒô JSON (array), bez ≈ºadnych komentarzy
   - Bez dodatkowych wyja≈õnie≈Ñ poza strukturƒÖ JSON
   - Bez blok√≥w markdown

---

### Wej≈õcie:
Podsumowanie dnia:
{news_summary_text}

### Oczekiwany wynik:
Zwr√≥ƒá wy≈ÇƒÖcznie tablicƒô JSON (array) zawierajƒÖcƒÖ obiekty ‚Äì ka≈ºdy reprezentuje osobny news:

[
  {{
    "typ": "Sp√≥≈Çka",
    "related_tickers": ["KGHM"],
    "sector": "surowce",
    "ticker_impact": 0.8,
    "sector_impact": null,
    "confidence": 0.9,
    "occasion": "≈õrednioterminowa",
    "reason": "Ceny miedzi wzros≈Çy po ograniczeniu eksportu z Chile, co sprzyja KGHM.",
    "brokerage_house": null,
    "price_old": null,
    "price_new": null,
    "price_recomendation": null,
    "price_comment": null
  }},
  {{
    "typ": "Sektor",
    "related_tickers": [],
    "sector": "banki",
    "ticker_impact": null,
    "sector_impact": -0.6,
    "confidence": 0.8,
    "occasion": null,
    "reason": "NBP zapowiedzia≈Ç mo≈ºliwo≈õƒá obni≈ºki st√≥p, co ogranicza mar≈ºe odsetkowe bank√≥w.",
    "brokerage_house": null,
    "price_old": null,
    "price_new": null,
    "price_recomendation": null,
    "price_comment": null
  }}
]
"""

def analyze_summary(headline, lead):
    """
    Analizuje podsumowanie dnia (mo≈ºe zawieraƒá wiele news√≥w) za pomocƒÖ OpenAI API.

    Args:
        headline: Tytu≈Ç artyku≈Çu
        lead: Tre≈õƒá/lead artyku≈Çu (podsumowanie wielu news√≥w)

    Returns:
        JSON string z listƒÖ analiz (array)
    """
    news_summary_text = f"{headline}\n\n{lead}"
    ticker_context = normalizer.get_prompt_context()
    prompt = PROMPT_SUMMARY_FIXED.format(news_summary_text=news_summary_text, ticker_context=ticker_context)

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        # UWAGA: Dla tablicy JSON nie u≈ºywamy response_format
        # bo wymusza to zwracanie obiektu, nie array
    )
    return response.choices[0].message.content

def analyze_news(headline, lead):
    """
    Analizuje pojedynczy news za pomocƒÖ OpenAI API.

    Args:
        headline: Tytu≈Ç artyku≈Çu
        lead: Tre≈õƒá/lead artyku≈Çu

    Returns:
        JSON string z wynikiem analizy
    """
    ticker_context = normalizer.get_prompt_context()
    prompt = PROMPT_NEWS.format(headline=headline, lead=lead, ticker_context=ticker_context)

    response = client.chat.completions.create(
        model="gpt-4o",  # Zaktualizowana nazwa modelu
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"}  # Wymu≈õ JSON
    )
    return response.choices[0].message.content


def get_unanalyzed_articles(db: Database, exclude_not_analyzed: bool = True):
    """
    Pobiera artyku≈Çy, kt√≥re nie majƒÖ jeszcze analizy.

    Args:
        db: Instancja Database
        exclude_not_analyzed: Czy wykluczyƒá artyku≈Çy z tabeli news_not_analyzed (domy≈õlnie True)

    Returns:
        Lista obiekt√≥w NewsArticle
    """
    session = db.Session()
    try:
        # Wybierz artyku≈Çy, kt√≥re nie majƒÖ wpisu w analysis_result
        query = session.query(NewsArticle).outerjoin(
            AnalysisResult, NewsArticle.id == AnalysisResult.news_id
        ).filter(AnalysisResult.id == None)

        # Opcjonalnie wykluczamy artyku≈Çy z news_not_analyzed
        if exclude_not_analyzed:
            query = query.outerjoin(
                NewsNotAnalyzed, NewsArticle.id == NewsNotAnalyzed.news_id
            ).filter(NewsNotAnalyzed.id == None)

        articles = query.order_by(NewsArticle.id.desc()).all()
        return articles
    finally:
        session.close()

def get_article_by_id(db: Database, article_id: int):
    """
    Pobiera artyku≈Ç po ID.

    Args:
        db: Instancja Database
        article_id: ID artyku≈Çu

    Returns:
        Obiekt NewsArticle lub None
    """
    session = db.Session()
    try:
        return session.query(NewsArticle).filter(NewsArticle.id == article_id).first()
    finally:
        session.close()


def is_article_analyzed(db: Database, article_id: int) -> bool:
    """
    Sprawdza czy artyku≈Ç zosta≈Ç ju≈º przeanalizowany.

    Args:
        db: Instancja Database
        article_id: ID artyku≈Çu

    Returns:
        True je≈õli artyku≈Ç ma ju≈º analizƒô, False w przeciwnym razie
    """
    session = db.Session()
    try:
        exists = session.query(AnalysisResult).filter(
            AnalysisResult.news_id == article_id
        ).first() is not None
        return exists
    finally:
        session.close()


def _save_single_analysis(session, news_id: int, analysis_data: dict, analysis_result_id: int):
    """
    Pomocnicza funkcja do zapisu pojedynczej analizy.

    Args:
        session: Sesja SQLAlchemy
        news_id: ID artyku≈Çu
        analysis_data: Dict z danymi analizy
        analysis_result_id: ID utworzonego rekordu AnalysisResult
    """
    # Pobierz pola z JSON
    related_tickers_raw = analysis_data.get('related_tickers', [])
    # Normalizuj tickery
    related_tickers = []
    for ticker_raw in related_tickers_raw:
        normalized_ticker, reason = normalizer.normalize(ticker_raw)
        if reason:
            print(f"Normalizacja tickera: {ticker_raw} -> {normalized_ticker} ({reason})")
        if normalized_ticker:
            related_tickers.append(normalized_ticker)
        else:
            print(f"Pominiƒôto nieznany ticker: {ticker_raw}")

    ticker_impact = analysis_data.get('ticker_impact')
    sector_impact = analysis_data.get('sector_impact')
    confidence_value = analysis_data.get('confidence')
    sector = analysis_data.get('sector')
    occasion = analysis_data.get('occasion')

    # Pola dla analiz dom√≥w maklerskich
    brokerage_house = analysis_data.get('brokerage_house')
    price_old = analysis_data.get('price_old')
    price_new = analysis_data.get('price_new')
    price_recommendation = analysis_data.get('price_recomendation')
    price_comment = analysis_data.get('price_comment')

    print(
        f"DEBUG: related_tickers={related_tickers}, ticker_impact={ticker_impact}, "
        f"sector_impact={sector_impact}, confidence={confidence_value}, sector={sector}, occasion={occasion}")

    # Najpierw dodaj tickery do s≈Çownika (je≈õli nie istniejƒÖ)
    for ticker_symbol in related_tickers:
        existing_ticker = session.query(Ticker).filter(
            Ticker.ticker == ticker_symbol).first()
        if not existing_ticker:
            print(f"DEBUG: Dodajƒô nowy ticker do s≈Çownika: {ticker_symbol}")
            new_ticker = Ticker(
                ticker=ticker_symbol,
                company_name=None,  # Mo≈ºe byƒá uzupe≈Çnione p√≥≈∫niej
                sector=sector
            )
            session.add(new_ticker)
        else:
            print(f"DEBUG: Ticker {ticker_symbol} ju≈º istnieje w s≈Çowniku")

    # Utw√≥rz ticker_sentiments (tylko je≈õli ticker_impact nie jest null)
    if related_tickers and ticker_impact is not None:
        for ticker_symbol in related_tickers:
            print(
                f"DEBUG: Dodajƒô ticker_sentiment dla {ticker_symbol} z ticker_impact={ticker_impact}, "
                f"confidence={confidence_value}, occasion={occasion}")
            ticker_sentiment = TickerSentiment(
                analysis_id=analysis_result_id,
                ticker=ticker_symbol,
                sector=sector,
                impact=ticker_impact,  # Float z ticker_impact
                confidence=confidence_value,  # Confidence (0.0-1.0)
                occasion=occasion  # Typ okazji
            )
            session.add(ticker_sentiment)

    # Dodaj sector_sentiment (tylko je≈õli sector_impact nie jest null)
    if sector and sector_impact is not None:
        print(
            f"DEBUG: Dodajƒô sector_sentiment dla sektora: {sector} z sector_impact={sector_impact}, "
            f"confidence={confidence_value}")
        sector_sentiment = SectorSentiment(
            analysis_id=analysis_result_id,
            sector=sector,
            impact=sector_impact,  # Float z sector_impact
            confidence=confidence_value  # Confidence (0.0-1.0)
        )
        session.add(sector_sentiment)

    # Dodaj BrokerageAnalysis (tylko je≈õli brokerage_house nie jest puste/null)
    if brokerage_house:
        # Je≈õli jest brokerage_house, powinien byƒá co najmniej jeden ticker
        ticker_for_brokerage = related_tickers[0] if related_tickers else None
        print(
            f"DEBUG: Dodajƒô BrokerageAnalysis: {brokerage_house} dla {ticker_for_brokerage}")
        brokerage_analysis = BrokerageAnalysis(
            analysis_id=analysis_result_id,
            ticker=ticker_for_brokerage,
            brokerage_house=brokerage_house,
            price_old=price_old,
            price_new=price_new,
            price_recommendation=price_recommendation,
            price_comment=price_comment
        )
        session.add(brokerage_analysis)


def save_analysis_results(db: Database, news_id: int, analysis_json: str):
    """
    Zapisuje wyniki analizy do bazy danych.
    Obs≈Çuguje zar√≥wno pojedynczƒÖ analizƒô (obiekt JSON), jak i listƒô analiz (array JSON).

    Args:
        db: Instancja Database
        news_id: ID artyku≈Çu
        analysis_json: JSON string z wynikiem analizy (obiekt lub array)

    Returns:
        ID utworzonego rekordu AnalysisResult (dla pojedynczej analizy)
        lub lista ID (dla listy analiz)
    """
    session = db.Session()
    try:
        # Usu≈Ñ potencjalny blok markdown z JSON
        cleaned_json = cleanJson(analysis_json)

        # Parsuj JSON
        print(f"DEBUG: Parsing JSON: {cleaned_json[:200]}...")
        analysis_data = json.loads(cleaned_json)

        # Sprawd≈∫ czy analysis_data jest listƒÖ (podsumowanie) czy pojedynczym obiektem (pojedynczy news)
        if isinstance(analysis_data, list):
            print(f"DEBUG: Wykryto listƒô analiz ({len(analysis_data)} element√≥w)")
            # To jest lista analiz - podsumowanie dnia
            analysis_ids = []
            for idx, single_analysis in enumerate(analysis_data):
                print(f"DEBUG: Przetwarzam analizƒô {idx + 1}/{len(analysis_data)}")

                # Utw√≥rz osobny wpis w analysis_result dla ka≈ºdej analizy
                analysis_result = AnalysisResult(
                    news_id=news_id,
                    summary=json.dumps(single_analysis, ensure_ascii=False)
                )
                session.add(analysis_result)
                session.flush()  # Aby uzyskaƒá ID
                print(f"DEBUG: Utworzono AnalysisResult z ID={analysis_result.id}")

                # Zapisz pojedynczƒÖ analizƒô
                _save_single_analysis(session, news_id, single_analysis, analysis_result.id)
                analysis_ids.append(analysis_result.id)

            session.commit()
            print(f"DEBUG: Commit wykonany pomy≈õlnie - zapisano {len(analysis_ids)} analiz")
            return analysis_ids  # Zwr√≥ƒá listƒô ID
        else:
            print(f"DEBUG: Wykryto pojedynczƒÖ analizƒô")
            # To jest pojedyncza analiza
            analysis_result = AnalysisResult(
                news_id=news_id,
                summary=cleaned_json
            )
            session.add(analysis_result)
            session.flush()  # Aby uzyskaƒá ID
            print(f"DEBUG: Utworzono AnalysisResult z ID={analysis_result.id}")

            # Zapisz pojedynczƒÖ analizƒô
            _save_single_analysis(session, news_id, analysis_data, analysis_result.id)

            session.commit()
            print(f"DEBUG: Commit wykonany pomy≈õlnie")
            return analysis_result.id
    except json.JSONDecodeError as e:
        session.rollback()
        raise ValueError(f"Nie mo≈ºna sparsowaƒá JSON: {e}")
    except Exception as e:
        session.rollback()
        raise e
    finally:
        session.close()


def cleanJson(analysis_json: str) -> str:
    """
    Czy≈õci JSON z markdown blok√≥w i dodatkowych komentarzy.
    
    Args:
        analysis_json: Surowy string JSON z odpowiedzi API
        
    Returns:
        Wyczyszczony string JSON
    """
    cleaned_json = analysis_json.strip()
    
    # Usu≈Ñ markdown bloki (```json ... ```)
    if cleaned_json.startswith('```'):
        lines = cleaned_json.split('\n')
        # Znajd≈∫ poczƒÖtek i koniec bloku
        start_idx = 0
        end_idx = len(lines)
        
        for i, line in enumerate(lines):
            if line.strip().startswith('```'):
                if start_idx == 0:
                    start_idx = i + 1
                else:
                    end_idx = i
                    break
        
        cleaned_json = '\n'.join(lines[start_idx:end_idx])
    
    # Usu≈Ñ komentarze po JSON (wszystko po zamykajƒÖcym } lub ])
    # Szukamy ostatniego } lub ] kt√≥ry ko≈Ñczy g≈Ç√≥wnƒÖ strukturƒô
    cleaned_json = cleaned_json.strip()
    
    # Sprawd≈∫ czy to array czy obiekt
    if cleaned_json.startswith('['):
        # Dla array, szukamy ostatniego ]
        last_bracket = cleaned_json.rfind(']')
        if last_bracket != -1:
            cleaned_json = cleaned_json[:last_bracket + 1]
    else:
        # Dla obiektu, szukamy ostatniego }
        last_brace = cleaned_json.rfind('}')
        if last_brace != -1:
            cleaned_json = cleaned_json[:last_brace + 1]
    
    return cleaned_json.strip()


def analyze_articles(db: Database, mode: str = 'unanalyzed', article_id: int = None,
                     relevance_threshold: float = 0.50, telegram=None, skip_relevance_check: bool = False):
    """
    G≈Ç√≥wna funkcja do analizy artyku≈Ç√≥w z wstƒôpnƒÖ filtracjƒÖ istotno≈õci.

    Args:
        db: Instancja Database
        mode: 'id' (dla konkretnego ID) lub 'unanalyzed' (dla nieprzeanalizowanych)
        article_id: ID artyku≈Çu (wymagane gdy mode='id')
        relevance_threshold: Pr√≥g istotno≈õci dla embeddings (0-1)
        telegram: Instancja Telegram do wysy≈Çania powiadomie≈Ñ
        skip_relevance_check: Je≈õli True, pomija sprawdzanie wzorc√≥w i od razu analizuje przez AI

    Returns:
        Dict z informacjƒÖ o przetworzonych artyku≈Çach
    """
    articles = []

    if mode == 'id':
        if article_id is None:
            raise ValueError("Dla trybu 'id' musisz podaƒá article_id")
        print(f"Szukam artyku≈Çu o ID={article_id}...")
        article = get_article_by_id(db, article_id)
        if article:
            articles = [article]
            print(f"Znaleziono artyku≈Ç: {article.title[:80]}")
        else:
            print(f"Nie znaleziono artyku≈Çu o ID={article_id}")
            return {"status": "error",
                    "message": f"Nie znaleziono artyku≈Çu o ID={article_id}"}
    elif mode == 'unanalyzed':
        print("Szukam nieprzeanalizowanych artyku≈Ç√≥w...")
        articles = get_unanalyzed_articles(db)
        print(f"Znaleziono {len(articles)} nieprzeanalizowanych artyku≈Ç√≥w")
    else:
        raise ValueError(f"Nieprawid≈Çowy tryb: {mode}. U≈ºyj 'id' lub 'unanalyzed'")

    if not articles:
        print("Brak artyku≈Ç√≥w do analizy")
        return {"status": "success", "message": "Brak artyku≈Ç√≥w do analizy",
                "not_relevant" : 0,
                "analyzed": 0}

    results = []
    analysis_json = None
    for article in articles:
        try:
            print(f"\n=== Przetwarzam artyku≈Ç ID={article.id}: {article.title[:50]}...")

            # Sprawd≈∫ czy artyku≈Ç ju≈º zosta≈Ç przeanalizowany
            if is_article_analyzed(db, article.id):
                print(
                    f"‚äò Artyku≈Ç ID={article.id} zosta≈Ç ju≈º wcze≈õniej przeanalizowany - pomijam")
                results.append({
                    "article_id": article.id,
                    "title": article.title,
                    "status": "skipped",
                    "reason": "already_analyzed"
                })
                continue

            # NOWE: Wstƒôpna analiza istotno≈õci (POMIJANA je≈õli skip_relevance_check=True)
            if skip_relevance_check:
                print(f"[1/2] Pomijam sprawdzanie wzorc√≥w - bezpo≈õrednia analiza AI...")
                has_summary = False
                is_relevant = True
                relevance_score = 1.0
            else:
                print(f"[1/3] Sprawdzam istotno≈õƒá newsa...")

                # Sprawd≈∫ czy news zawiera negatywne s≈Çowa kluczowe
                has_negative, negative_keyword = contains_pattern(NEGATIVE_KEYWORDS, article.title, article.content or "")
                if has_negative:
                    reason = f"Zawiera negatywne s≈Çowo kluczowe: '{negative_keyword}'"
                    print(f"    ‚úó Wykluczony: {reason}")
                    save_not_analyzed(db, article.id, reason, 0.0)
                    results.append({
                        "article_id": article.id,
                        "title": article.title,
                        "status": "skipped",
                        "reason": "negative_keyword",
                        "relevance_score": 0.0,
                        "details": reason
                    })
                    continue
                has_summary, summary_keyword = contains_pattern(NEWS_SUMMARY_PATTERN,
                                                                  article.title,
                                                                  article.content or "")
                if not has_summary:
                    is_relevant, relevance_score, relevance_reason = is_news_relevant(
                        article.title,
                        article.content or "",
                        threshold=relevance_threshold
                    )
                else:
                    is_relevant, relevance_score, relevance_reason = True, 1, "Podsumowanie dnia"
                    print(f"    Pomijam, Pow√≥d: Podsumowanie dnia")
                    continue

                print(
                    f"    Istotno≈õƒá: {'TAK' if is_relevant else 'NIE'} (score: {relevance_score:.3f})")
                print(f"    Pow√≥d: {relevance_reason}")

                if not is_relevant:
                    # Zapisz do news_not_analyzed
                    save_not_analyzed(db, article.id, relevance_reason, relevance_score)
                    results.append({
                        "article_id": article.id,
                        "title": article.title,
                        "status": "skipped",
                        "reason": "not_relevant",
                        "relevance_score": relevance_score,
                        "details": relevance_reason
                    })
                    continue

            # Analizuj artyku≈Ç (tylko je≈õli jest istotny lub skip_relevance_check=True)
            step_num = "[2/2]" if skip_relevance_check else "[2/3]"
            print(f"{step_num} Wysy≈Çam zapytanie do OpenAI...")
            if has_summary:
                analysis_json = analyze_summary(article.title, article.content or "")
                analysis_datas = json.loads(cleanJson(analysis_json))
                for analysis_data in analysis_datas:
                    tickers = analysis_data.get('related_tickers', [])
                    ticker_impact = analysis_data.get('ticker_impact')
                    sector = analysis_data.get('sector')
                    sector_impact = analysis_data.get('sector_impact')
                    if tickers and ticker_impact and ticker_impact != 0:
                        telegram.send_analysis_alert(ticker=','.join(tickers),
                                                     title=article.title,
                                                     reason=analysis_data.get('reason'),
                                                     impact=ticker_impact,
                                                     confidence=analysis_data.get(
                                                         'confidence')
                                                     )
                    elif sector and sector_impact:
                        telegram.send_sector_alert(sector=sector,
                                                   title=article.title,
                                                   reason=analysis_data.get('reason'),
                                                   impact=sector_impact,
                                                   confidence=analysis_data.get(
                                                       'confidence')
                                                   )

            else:
                analysis_json = analyze_news(article.title, article.content or "")
                analysis_data = json.loads(cleanJson(analysis_json))
                tickers = analysis_data.get('related_tickers', [])
                sector_impact = analysis_data.get('sector_impact')
                sector = analysis_data.get('sector')
                if tickers and telegram:
                    telegram.send_analysis_alert(ticker=','.join(tickers),
                                                 title=article.title,
                                                 reason=analysis_data.get('reason'),
                                                 impact=analysis_data.get('ticker_impact'),
                                                 confidence=analysis_data.get('confidence')
                                                 )
                elif sector and telegram:
                    telegram.send_sector_alert(sector=sector,
                                                 title=article.title,
                                                 reason=analysis_data.get('reason'),
                                                 impact=sector_impact,
                                                 confidence=analysis_data.get('confidence')
                                                 )
            print(f"    Otrzymano odpowied≈∫: {analysis_json[:100]}...")

            # Zapisz wyniki
            step_num = "[2/2]" if skip_relevance_check else "[3/3]"
            print(f"{step_num} Zapisujƒô wyniki do bazy danych...")

            analysis_id = save_analysis_results(db, article.id, analysis_json)
            print(f"‚úì Pomy≈õlnie zapisano analizƒô (analysis_id={analysis_id})")

            results.append({
                "article_id": article.id,
                "analysis_id": analysis_id,
                "title": article.title,
                "status": "success",
                "relevance_score": relevance_score
            })
        except Exception as e:
            print(f"‚úó B≈ÅƒÑD podczas analizy artyku≈Çu ID={article.id}, json={analysis_json}: {str(e)}")
            import traceback
            traceback.print_exc()
            results.append({
                "article_id": article.id,
                "title": article.title,
                "status": "error",
                "error": str(e)
            })

    success_count = sum(1 for r in results if r['status'] == 'success')
    skipped_count = sum(1 for r in results if r['status'] == 'skipped')
    not_relevant_count = sum(1 for r in results if r.get('reason') == 'not_relevant')
    error_count = sum(1 for r in results if r['status'] == 'error')

    print(f"\n{'=' * 60}")
    print(f"PODSUMOWANIE:")
    print(f"  Przeanalizowane:     {success_count}")
    print(f"  Pominiƒôte (ju≈º by≈Çy): {skipped_count - not_relevant_count}")
    print(f"  Odrzucone (nieistotne): {not_relevant_count}")
    print(f"  B≈Çƒôdy:               {error_count}")
    print(f"{'=' * 60}\n")

    return {
        "status": "completed",
        "analyzed": success_count,
        "skipped": skipped_count - not_relevant_count,
        "not_relevant": not_relevant_count,
        "errors": error_count,
        "results": results
    }


def calculate_trends(news_list):
    """
    Oblicza trendy sektorowe na podstawie ocen news√≥w.
    Ka≈ºdy element listy powinien mieƒá pola:
    - 'sector': str
    - 'impact': float  (od -1 do 1)
    - 'confidence': float  (od 0 do 1)
    """

    # grupowanie po sektorach
    sectors = defaultdict(list)
    for n in news_list:
        if n.get("sector") and n.get("impact") is not None:
            weighted = n["impact"] * n.get("confidence", 1.0)
            sectors[n["sector"]].append(weighted)

    # liczymy ≈õredni trend dla ka≈ºdego sektora
    summary = []
    for sector, weights in sectors.items():
        avg = sum(weights) / len(weights)

        # klasyfikacja trendu
        if avg > 0.15:
            momentum = "rosnƒÖce"
        elif avg < -0.15:
            momentum = "malejƒÖce"
        else:
            momentum = "neutralne"

        summary.append({
            "sector": sector,
            "trend_score": round(avg, 3),
            "momentum": momentum,
            "count": len(weights)
        })

    # sortowanie po sile trendu (od najwy≈ºszego do najni≈ºszego)
    summary.sort(key=lambda x: x["trend_score"], reverse=True)
    return summary


def get_sector_report(db: Database):
    """
    Generuje raport trend√≥w dla sektor√≥w na podstawie danych z tabeli sector_sentiment.

    Args:
        db: Instancja Database

    Returns:
        Lista s≈Çownik√≥w z trendami sektorowymi
    """
    session = db.Session()
    try:
        # Pobierz wszystkie wpisy z sector_sentiment
        sentiments = session.query(SectorSentiment).all()

        # Przekszta≈Çƒá do formatu wymaganego przez calculate_trends
        news_list = []
        for sentiment in sentiments:
            if sentiment.sector and sentiment.impact is not None:
                try:
                    impact_value = float(sentiment.impact)
                    confidence_value = sentiment.confidence if sentiment.confidence is not None else 1.0

                    news_list.append({
                        "sector": sentiment.sector,
                        "impact": impact_value,
                        "confidence": confidence_value
                    })
                except (ValueError, TypeError):
                    # Pomi≈Ñ nieprawid≈Çowe warto≈õci
                    continue

        # U≈ºyj calculate_trends do obliczenia raport
        return calculate_trends(news_list)
    finally:
        session.close()


def get_ticker_report(db: Database):
    """
    Generuje raport trend√≥w dla ticker√≥w na podstawie danych z tabeli ticker_sentiment.

    Args:
        db: Instancja Database

    Returns:
        Lista s≈Çownik√≥w z trendami dla ticker√≥w
    """
    session = db.Session()
    try:
        # Pobierz wszystkie wpisy z ticker_sentiment
        sentiments = session.query(TickerSentiment).all()

        # Grupowanie po tickerach
        tickers = defaultdict(list)
        for sentiment in sentiments:
            if sentiment.ticker and sentiment.impact is not None:
                try:
                    impact_value = float(sentiment.impact)
                    confidence_value = sentiment.confidence if sentiment.confidence is not None else 1.0
                    weighted = impact_value * confidence_value
                    tickers[sentiment.ticker].append(weighted)
                except (ValueError, TypeError):
                    continue

                # Liczymy ≈õredni trend dla ka≈ºdego tickera
                summary = []
                for ticker, weights in tickers.items():
                    avg = sum(weights) / len(weights)

                    # Klasyfikacja trendu
                    if avg > 0.15:
                        momentum = "pozytywny"
                    elif avg < -0.15:
                        momentum = "negatywny"
                    else:
                        momentum = "neutralny"

                    ticker, reason = normalizer.normalize(ticker)

                    if reason:
                        print(f"Normalizacja tickera: {reason}")

                    summary.append({
                        "ticker": ticker,
                        "trend_score": round(avg, 3),
                        "momentum": momentum,
                        "count": len(weights)
                    })

                # Sortowanie po sile trendu
                summary.sort(key=lambda x: x["trend_score"], reverse=True)
                return summary

    finally:
        session.close()

def generate_report(db: Database):
    """
    Generuje pe≈Çny raport zawierajƒÖcy trendy dla sektor√≥w i ticker√≥w.

    Args:
        db: Instancja Database

    Returns:
        Dict z raportami dla sektor√≥w i ticker√≥w
    """
    print("\n" + "="*60)
    print("GENEROWANIE RAPORTU ANALIZ")
    print("="*60)

    # Raport dla sektor√≥w
    print("\n[1/2] Generujƒô raport dla sektor√≥w...")
    sector_report = get_sector_report(db)
    print(f"‚úì Znaleziono {len(sector_report)} sektor√≥w")

    # Raport dla ticker√≥w
    print("\n[2/2] Generujƒô raport dla sp√≥≈Çek (ticker√≥w)...")
    ticker_report = get_ticker_report(db)
    print(f"‚úì Znaleziono {len(ticker_report)} ticker√≥w")

    report = {
        "sectors": sector_report,
        "tickers": ticker_report
    }

    # Wy≈õwietl podsumowanie
    print("\n" + "="*60)
    print("RAPORT SEKTOR√ìW")
    print("="*60)
    if sector_report:
        for sector in sector_report[:10]:  # Top 10
            print(f"{sector['sector']:20} | Score: {sector['trend_score']:+6.3f} | "
                  f"Momentum: {sector['momentum']:12} | Liczba: {sector['count']}")
    else:
        print("Brak danych dla sektor√≥w")

    print("\n" + "="*60)
    print("RAPORT SP√ì≈ÅEK (TOP 20)")
    print("="*60)
    if ticker_report:
        for ticker in ticker_report[:20]:  # Top 20
            print(f"{ticker['ticker']:10} | Score: {ticker['trend_score']:+6.3f} | "
                  f"Momentum: {ticker['momentum']:12} | Liczba: {ticker['count']}")
    else:
        print("Brak danych dla ticker√≥w")

    print("\n" + "="*60)

    return report

if __name__ == "__main__":
    """
    Przyk≈Çad u≈ºycia:

    # Tryb 1: Analiza konkretnego artyku≈Çu po ID
    db = Database('news.db')
    result = analyze_articles(db, mode='id', article_id=123)
    print(result)

    # Tryb 2: Analiza wszystkich nieprzeanalizowanych artyku≈Ç√≥w
    db = Database('news.db')
    result = analyze_articles(db, mode='unanalyzed')
    print(result)
    """
    import sys
    from backend.config import Config

    config = Config()
    db = Database()

    if len(sys.argv) > 1:
        if sys.argv[1] == '--id' and len(sys.argv) > 2:
            # Analiza konkretnego artyku≈Çu
            article_id = int(sys.argv[2])
            print(f"Analizujƒô artyku≈Ç ID={article_id}...")
            result = analyze_articles(db, mode='id', article_id=article_id)
            print(result)
        elif sys.argv[1] == '--unanalyzed':
            # Analiza nieprzeanalizowanych
            print("Analizujƒô nieprzeanalizowane artyku≈Çy...")
            result = analyze_articles(db, mode='unanalyzed')
            #print(result)
        elif sys.argv[1] == '--report':
            # Generuj raport
            report = generate_report(db)
        else:
            print("U≈ºycie:")
            print("  python ai_analist.py --id <article_id>  # Analizuj konkretny artyku≈Ç")
            print("  python ai_analist.py --unanalyzed       # Analizuj wszystkie nieprzeanalizowane")
            print("  python ai_analist.py --report           # Generuj raport trend√≥w")
    else:
        print("U≈ºycie:")
        print("  python ai_analist.py --id <article_id>  # Analizuj konkretny artyku≈Ç")
        print("  python ai_analist.py --unanalyzed       # Analizuj wszystkie nieprzeanalizowane")
        print("  python ai_analist.py --report           # Generuj raport trend√≥w")
